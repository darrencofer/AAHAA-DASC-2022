\section{Autonomy Framework}

[MATT/DENIS - 1.5 pg total]

In this work, the corrective maneuver for the avoidance is generated by a pre-computed solution -- Reinforcement Learning (RL) policy model. 
The RL is a sequential policy optimization method that solves the task using the ''learning by doing'' conception and requires continuous data-rich interaction with the environment \cite{sutton2018reinforcement}. 
Our avoidance policy is trained on a surrogate task to provide a required number of interactions. 
This surrogate environment developed by Boeing is a lightweight Python environment integrated with the OpenAI GYM framework \cite{brockman2016openai}.

The learned policy minimizes the risk of collision by providing continuous control commands in the surrogate environment. These commands are converted into a geometric trajectory using the surrogate environment and Boeing-provided ROS interfaces.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figures/system_overview.pdf}
	\caption{System diagram of the RL model-based conflict resolver.}
	\label{fig:diagram}
\end{figure}

\subsection{RL Training}

Our surrogate environment is a simplified 2D obstacle avoidance problem that mimics the real task (air traffic conflict resolution). 
The environment simulates the movements of two aircraft on 20x20 kilometers square. The controlled Agent has to go around the Intruder, provide minimal horizontal separation, and merge back to the next safe waypoint from the original route before the simulation ends.

\begin{table}[h]
	\centering
	\begin{tabular}{||c | c | c||} 
		\hline
		Parameter & Range & Units \\
		\hline
		Time step $dT$ & 1.0  & sec \\
		Max time $t_\text{MAX}$ & 300.0  & sec \\
		Distance range & -10000 .. 10000  & m \\
		Horizontal separation & 1000  & m \\
		\hline
	\end{tabular}
	\caption{Surrogate environment simulation parameters.}
	\label{tbl:sim_param}
\end{table}


The surrogate dynamics imitate the dynamics of the vehicle selected for the experiment -- a single-engine turboprop Cessna 208 Grand Caravan.

All the vehicles are represented by a simplistic dynamic model:
\begin{itemize}
	\item Dubin's vehicle model for turn dynamics
	\item Mass-less kinematics model 
	\end {itemize}
	
	
	All the observation and control values are normalized to [-1..1] for the RL agent.
	
	To make sure the agent generalizes the problem, we rewrap the observations and focus only on relative positions rather than absolute. In addition, we normalize the values to [-1 .. 1].
	Repacked observations consumed by the agent:
	\begin{center}
		\begin{tabular}{|| c | c ||} 
			\hline
			heading & intruder heading\\
			airspeed & intruder airspeed\\
			distance to goal & distance to intruder\\
			tracking angle to goal & tracking angle to intruder\\
			\hline
		\end{tabular}
	\end{center}

\subsection{RL Policy Agent}

The RL policy agent learns the task by interacting with the simulation and iteratively updating the parameters of the policy model using Stochastic Gradient Descent (SGD) optimization. We approximate the policy with a multi-layered perceptron shown in Fig. \ref{fig:policy_model}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/model.pdf}
	\caption{Neural network function approximation used for the policy model consists of 2 hidden layers, 256 neurons each.}
	\label{fig:policy_model}
\end{figure}

To solve the optimization problem as Markov Decision Problem (MDP) system, we refactor it into Markovian states $s$, transitions  $T(s' | s, a)$, and transition reward $R(s' | s, a)$. 
The state of the system (including both Agent and Intruder) is fully observable, assumes the perfect knowledge, and is enough to describe the Markovian state of the MDP system.
State of the agent described as
$$ s = \{ v_a, \psi_a, v_i, \psi_i, \beta_i, d_i, \beta_g, d_g \}$$

where 
$v_a$ - agent speed,
$\psi_a$ - agent heading,
$v_i$ - intruder speed,
$\psi_i$ - intruder heading,
$\beta_i$ - angle to intruder,
$d_i$ - distance to intruder,
$\beta_g$ - angle to goal,
$d_g$ - distance to goal. 

The optimization is set to find the optimal policy $\pi^*(s)$ as a set of state-action mappings that maximizes the expected reward $V(s)$ \cite{sutton2018reinforcement}.
\begin{align} 
	\pi(s) &= P(a | s) \\
	\pi^*(s) &= \arg\max_{\pi} V^\pi (s) \\
	&= \arg\max_a \left( R(s,a) + \gamma T(s'|s,a) V(s') \right)
\end{align} 

Value of the state is expected future reward accumulated over the trajectory and defined by Bellman function as:
\begin{align}
	V(s) &=  \mathbb{E} [R | s, \pi] \\
	&= \sum_{s'} T(s'|s,a) \left( R(s'|s,a) + \gamma ( V(s')) \right) \\
	&=  R(s'|s,a) + \gamma \sum_{s'} T(s'|s,a) V^{\pi}(s') \\
	V^{*}(s) &= \max_{a} \left( R(s,a) + \gamma \sum_{s'} T(s'|s,a) V^{*}(s') \right)
\end{align}

The RL policy model is based on the Actor-Critic architecture that helps to improve the stability of the training \cite{sutton2018reinforcement}. The SGD-based update for Actor $\theta$ and Critic $w$ networks:
\begin{align}
	\delta &=  R_{t+1} +\gamma \hat V(s_{t+1},w) - \hat V(s_t,w) \\
	w &\leftarrow w + \alpha \delta \nabla \hat V (s, w) \\
	\theta &\leftarrow \theta + \alpha \delta \nabla \ln \pi (a|s, \theta)
\end{align} 

The core functionality of the RL agent incorporates the Stable Baselines library, a very reputable fork of OpenAI Baselines \cite{hill2018stable}. For the exploration policy and update steps, this work used the Proximal Policy Optimization (PPO) algorithm that becomes the state of the art in continuous-action agents \cite{schulman2017proximal}.

[MATT/JIM] Overview of the Boeing autonomy framework and aircraft used to demonstrate the collision avoidance neural network capability.

Collision avoidance problem -- Strategic rather than tactical, avoidance flight plan must avoid intruder but return to original flight plan

Limitations --  two-dimensional, lateral avoidance maneuvers, focus on single intruder

Describe the safety requirements for collision avoidance.  Maybe reference DO-365. 

Experiments are carried out leveraging the Boeing Autonomy Testbed Aircraft in the form of a Cessna Caravan 208B, tail number N208BX, and is currently serving as a test bed for the DARPA Assured Autonomy program’s air domain (\ref{fig:caravan}).  See Figure X.  The Testbed is optionally piloted and serves as a means to demonstrate commercially viable technologies leading to autonomy.  It is a research and development vehicle able to operate in commercial airspace that is built on open-source middleware with in-house developed guidance and control technologies leveraged from across the Boeing enterprise.  The Testbed includes a full Iron Bird fixture in which new autonomy technology can be fully integrated and tested before flight.  With this Testbed Boeing has demonstrated autonomy firsts including in-air detect and avoid, ADS-B transponder-based route planning for strategic avoidance, and fully autonomous ground taxi.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/caravan.jpg}
	\caption{Boeing Autonomy Testbed Aircraft - Cessna Caravan 208B}
	\label{fig:caravan}
\end{figure}

Detect and avoid mission operating and performance standards are defined in RTCA document DO-365 and includes regulatory guidance for UAS interactions with the National Airspace system, requirement safe operation of aircraft during encounters including separation distance minimums for remaining well clear of aircraft and mid-air collision avoidance, and proper aircraft equipages to achieve safe detect and avoid operations.
The assurance challenged posed herein focuses on the Autonomy Testbed aircraft flying in the vicinity of another intruding airplane, where the Test flight software includes a Boeing-developed LEC to generate an avoidance trajectory for the Testbed to remain well clear of the intruder aircraft as defined in DO-365.  The underlying assurance technology montors the Boeing LEC in order to assess the avoidance trajectory from the LEC.
Figure X shows a detailed view of the autonomy system framework as deployed on the Testbed aircraft.

System elements include: 
\begin{itemize}
\item Perception Sensor – Primary sensor for perceiving the airspace is the Automated Dependent Surveillance Broadcast (ADS-B) providing detection information to the other system functions.
\item ADS-B Tracker – Generates intruder track information from received ADS-B messages to the other system functions.
\item Avoidance Assessment – Evaluates potential future traffic conflicts and issues “alerts” to the Avoidance functions.  Assessment definition and requirements are specified in DO-365 MOPS for DAA System.
\item Baseline (Non AI/ML) Avoidance Guidance – Provides waypoint navigation paths to avoid airspace incursion.  Avoidance computation method is virtual predictive radar which is designed to provide maximum “safe passage timed corridors.”  Avoidance path terminates back on original flight plan.
\item Runtime Assurance Function – SW block representing runtime monitoring and assurance functions concerning LEC and CPS behavior.
\item Autonomous Executive – Constructs and manages execution of the vehicle flight path and contains a SW node to splice in avoidance guidance waypoint paths into the original flight path.
\item Contingency Manager – Maps faults to autonomous actions.  Acts as a deterministic switch between complex functions and recovery actions in reference to ATSM 3269-17 standard.
\item Vehicle Manager – Executes flight path provided by AE including guidance and control for the vehicle.  The VM also sends commands and receives feedback from actuation system components.
\item Actuators / Sensors – Carries out VMS flight control surface commands and provides positional feedback.
\item Ownship State Estimation – Provides vehicle state information including position, altitude, and speed along with the vehicle inertial reference frame.
\item Navigation Database – Reference database of aircraft parameters and airspace waypoints, terrain, airports, approaches, etc.  Used for route/avoidance planning purposes.
\end{itemize}

[DENIS/DRAGOS]

\subsection{ROS Integration}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figures/cp25_ros.pdf}
	\caption{System diagram of the ROS-LEC (ROS-Agent) runner -- the integration of the learning-enables component (LEC) to the physical demonstrator using ROS interface.}
	\label{fig:integration}
\end{figure}

CAS system integration is done using Robot Operating System (ROS) interface \cite{quigley2009ros}. This allows unifying the interfaces to the high-fidelity simulation and to the physical demonstrator.
ROS-Agent (ROS-LEC) runner, demonstrated in Fig. \ref{fig:integration}, aggregates data from different domains and provides important utilities to the system. Its job is designed as follows: 
\begin{itemize}
	\item receive and accumulate ROS messages regarding the own-ship state,
	Intruder's state, traffic alerts, GPS-SRS transformation data,
	\item translate ADSB and GPS-Novatel positioning data to local coordinate frame,
	\item extract the goal location from the original route,
	\item re-wrap the observations into the Agent-specific input format,
	\item iteratively run the Agent to get the corrective actions,
	\item iteratively run the surrogate environment to receive the transitions,
	\item form a corrective trajectory and check if the trajectory is good,
	\item translate the trajectory from local coordinate frame to global lat-long waypoints,
	\item publish the trajectory as ROS message.
\end{itemize}

On an external request, the runner generates a single avoidance trajectory and publishes it as ROS message. The trajectory consists of 20 waypoints in total. The last waypoint is taken from the original route, and 19 waypoints are generated by the policy. This allows linking the waypoints by a unique index and preserving the indices of the original route.

These waypoints are spaced 20 seconds apart which provides a 400-second planning horizon. 
Because of the large time step between the waypoints, the policy and the surrogate have to be evaluated 20 times to make a single waypoint. The total response time of the system is below 60 msec for the complete trajectory.

When needed to re-plan, the runner can be requested again. This architecture allows closed-loop corrections with an external run-time assurance monitor. The monitor keeps track of the accumulated transition error and either requests an updated avoidance plan or denies the operation switching to a back-up mode.
