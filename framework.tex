\section{Autonomy Framework}

Overview

\input{caravan}

\subsection{LEC Training with Reinforcement Learning}
%Description of LEC and RL, ROS integration

In this work, the corrective maneuver for the avoidance is generated by a pre-computed solution -- Reinforcement Learning (RL) policy model. 
The RL is a sequential policy optimization method that solves the task using the ''learning by doing'' conception and requires continuous data-rich interaction with the environment \cite{sutton2018reinforcement}. 
Our avoidance policy is trained on a surrogate task to provide a required number of interactions. 
This surrogate environment developed by Boeing is a lightweight Python environment integrated with the OpenAI GYM framework \cite{brockman2016openai}.

The learned policy minimizes the risk of collision by providing continuous control commands in the surrogate environment. These commands are converted into a geometric trajectory using the surrogate environment and Boeing-provided ROS interfaces.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figures/system_overview.pdf}
	\caption{System diagram of the RL model-based conflict resolver.}
	\label{fig:diagram}
\end{figure}


Our surrogate environment is a simplified 2D obstacle avoidance problem that mimics the real task (air traffic conflict resolution). 
The environment simulates the movements of two aircraft on 20x20 kilometers square. The controlled Agent has to go around the Intruder, provide minimal horizontal separation, and merge back to the next safe waypoint from the original route before the simulation ends.

\begin{table}[h]
	\centering
	\begin{tabular}{||c | c | c||} 
		\hline
		Parameter & Range & Units \\
		\hline
		Time step $dT$ & 1.0  & sec \\
		Max time $t_\text{MAX}$ & 300.0  & sec \\
		Distance range & -10000 .. 10000  & m \\
		Horizontal separation & 1000  & m \\
		\hline
	\end{tabular}
	\caption{Surrogate environment simulation parameters.}
	\label{tbl:sim_param}
\end{table}


The surrogate dynamics imitate the dynamics of the vehicle selected for the experiment -- a single-engine turboprop Cessna 208 Grand Caravan.

All the vehicles are represented by a simplistic dynamic model:
\begin{itemize}
	\item Dubin's vehicle model for turn dynamics
	\item Mass-less kinematics model 
	\end {itemize}
	
	
	All the observation and control values are normalized to [-1..1] for the RL agent.
	
	To make sure the agent generalizes the problem, we rewrap the observations and focus only on relative positions rather than absolute. In addition, we normalize the values to [-1 .. 1].
	Repacked observations consumed by the agent:
	\begin{center}
		\begin{tabular}{|| c | c ||} 
			\hline
			heading & intruder heading\\
			airspeed & intruder airspeed\\
			distance to goal & distance to intruder\\
			tracking angle to goal & tracking angle to intruder\\
			\hline
		\end{tabular}
	\end{center}

\subsection{RL Policy Agent}

The RL policy agent learns the task by interacting with the simulation and iteratively updating the parameters of the policy model using Stochastic Gradient Descent (SGD) optimization. We approximate the policy with a multi-layered perceptron shown in Fig. \ref{fig:policy_model}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/model.pdf}
	\caption{Neural network function approximation used for the policy model consists of 2 hidden layers, 256 neurons each.}
	\label{fig:policy_model}
\end{figure}

To solve the optimization problem as Markov Decision Problem (MDP) system, we refactor it into Markovian states $s$, transitions  $T(s' | s, a)$, and transition reward $R(s' | s, a)$. 
The state of the system (including both Agent and Intruder) is fully observable, assumes the perfect knowledge, and is enough to describe the Markovian state of the MDP system.
State of the agent described as
$$ s = \{ v_a, \psi_a, v_i, \psi_i, \beta_i, d_i, \beta_g, d_g \}$$

where 
$v_a$ - agent speed,
$\psi_a$ - agent heading,
$v_i$ - intruder speed,
$\psi_i$ - intruder heading,
$\beta_i$ - angle to intruder,
$d_i$ - distance to intruder,
$\beta_g$ - angle to goal,
$d_g$ - distance to goal. 

The optimization is set to find the optimal policy $\pi^*(s)$ as a set of state-action mappings that maximizes the expected reward $V(s)$ \cite{sutton2018reinforcement}.
\begin{align} 
	\pi(s) &= P(a | s) \\
	\pi^*(s) &= \arg\max_{\pi} V^\pi (s) \\
	&= \arg\max_a \left( R(s,a) + \gamma T(s'|s,a) V(s') \right)
\end{align} 

Value of the state is expected future reward accumulated over the trajectory and defined by Bellman function as:
\begin{align}
	V(s) &=  \mathbb{E} [R | s, \pi] \\
	&= \sum_{s'} T(s'|s,a) \left( R(s'|s,a) + \gamma ( V(s')) \right) \\
	&=  R(s'|s,a) + \gamma \sum_{s'} T(s'|s,a) V^{\pi}(s') \\
	V^{*}(s) &= \max_{a} \left( R(s,a) + \gamma \sum_{s'} T(s'|s,a) V^{*}(s') \right)
\end{align}

The RL policy model is based on the Actor-Critic architecture that helps to improve the stability of the training \cite{sutton2018reinforcement}. The SGD-based update for Actor $\theta$ and Critic $w$ networks:
\begin{align}
	\delta &=  R_{t+1} +\gamma \hat V(s_{t+1},w) - \hat V(s_t,w) \\
	w &\leftarrow w + \alpha \delta \nabla \hat V (s, w) \\
	\theta &\leftarrow \theta + \alpha \delta \nabla \ln \pi (a|s, \theta)
\end{align} 

The core functionality of the RL agent incorporates the Stable Baselines library, a very reputable fork of OpenAI Baselines \cite{hill2018stable}. For the exploration policy and update steps, this work used the Proximal Policy Optimization (PPO) algorithm that becomes the state of the art in continuous-action agents \cite{schulman2017proximal}.



[DENIS/DRAGOS]

\subsection{ROS Integration}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figures/cp25_ros.pdf}
	\caption{System diagram of the ROS-LEC (ROS-Agent) runner -- the integration of the learning-enables component (LEC) to the physical demonstrator using ROS interface.}
	\label{fig:integration}
\end{figure}

CAS system integration is done using Robot Operating System (ROS) interface \cite{quigley2009ros}. This allows unifying the interfaces to the high-fidelity simulation and to the physical demonstrator.
ROS-Agent (ROS-LEC) runner, demonstrated in Fig. \ref{fig:integration}, aggregates data from different domains and provides important utilities to the system. Its job is designed as follows: 
\begin{itemize}
	\item receive and accumulate ROS messages regarding the own-ship state,
	Intruder's state, traffic alerts, GPS-SRS transformation data,
	\item translate ADSB and GPS-Novatel positioning data to local coordinate frame,
	\item extract the goal location from the original route,
	\item re-wrap the observations into the Agent-specific input format,
	\item iteratively run the Agent to get the corrective actions,
	\item iteratively run the surrogate environment to receive the transitions,
	\item form a corrective trajectory and check if the trajectory is good,
	\item translate the trajectory from local coordinate frame to global lat-long waypoints,
	\item publish the trajectory as ROS message.
\end{itemize}

On an external request, the runner generates a single avoidance trajectory and publishes it as ROS message. The trajectory consists of 20 waypoints in total. The last waypoint is taken from the original route, and 19 waypoints are generated by the policy. This allows linking the waypoints by a unique index and preserving the indices of the original route.

These waypoints are spaced 20 seconds apart which provides a 400-second planning horizon. 
Because of the large time step between the waypoints, the policy and the surrogate have to be evaluated 20 times to make a single waypoint. The total response time of the system is below 60 msec for the complete trajectory.

When needed to re-plan, the runner can be requested again. This architecture allows closed-loop corrections with an external run-time assurance monitor. The monitor keeps track of the accumulated transition error and either requests an updated avoidance plan or denies the operation switching to a back-up mode.
